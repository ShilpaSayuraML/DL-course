{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": "true",
        "id": "eVKixSdX1lw5"
      },
      "source": [
        "# Table of Contents\n",
        " <p><div class=\"lev1 toc-item\"><a href=\"#ANN-Intuition\" data-toc-modified-id=\"ANN-Intuition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ANN Intuition</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Neuron\" data-toc-modified-id=\"The-Neuron-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The Neuron</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Activation-Function\" data-toc-modified-id=\"The-Activation-Function-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The Activation Function</a></div><div class=\"lev2 toc-item\"><a href=\"#How-do-NNs-work?\" data-toc-modified-id=\"How-do-NNs-work?-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>How do NNs work?</a></div><div class=\"lev2 toc-item\"><a href=\"#How-do-Neural-Networks-learn?\" data-toc-modified-id=\"How-do-Neural-Networks-learn?-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>How do Neural Networks learn?</a></div><div class=\"lev2 toc-item\"><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Backpropagation</a></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqBIEgas1lxk"
      },
      "source": [
        "## The Neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_-jP-1j1lxm"
      },
      "source": [
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.26.10%20PM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4apixYg31lxo"
      },
      "source": [
        "__The Neuron__ contains:\n",
        "    - Neuron\n",
        "    - Dendrites: receiver of the neuron\n",
        "    - Axon: transmitter of the signal for the neuron\n",
        "    \n",
        "How can we represent neuron in machine?\n",
        "- Input signal: Dendrites\n",
        "- Output signal: Axon\n",
        "- Input values, their signals pass through Synapse to Neuron, then the neuron has an output value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ormrBoi61lxq"
      },
      "source": [
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.39.38%20PM.png?raw=1\">\n",
        "\n",
        "- Input layer:\n",
        "    - Receive all input values (independent variables). These independent variables are all for 1 single observation (1 row of all values: age, bank amount, ...)\n",
        "    - You should standardize these input values.\n",
        "    \n",
        "- Output value: can be\n",
        "    - Continous (price)\n",
        "    - Binary\n",
        "    - Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qv1vpls1lxr"
      },
      "source": [
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.46.10%20PM.png?raw=1\">\n",
        "\n",
        "Weights: are how neural networks learn.\n",
        "- By adjusting the weights, the neural network decides for every single case which signal is important, and which one is not.\n",
        "- Weights are adjusted through the process of learning.\n",
        "\n",
        "What happen inside the Neuron?\n",
        "- 1st step: $\\sum_{i=1}^m w_i x_i$, takes the weighted sum of all the input values.\n",
        "- 2nd step: apply the activation function ($\\phi$) to the weighted sum.\n",
        "- 3rd step: the neuron passes the signal to the next neuron down the line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-nX1Hky1lxt"
      },
      "source": [
        "## The Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcvg3S3q1lxu"
      },
      "source": [
        "There are more activation fucntions, but we are going to look at 4 different types of activation function.\n",
        "\n",
        "__Threshold function__\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%209.59.29%20PM.png?raw=1\">\n",
        "\n",
        "- If the value is less than 0, then the Threshold function passes on 0.\n",
        "- If the value is more than or equal to 0, then the Threshold function passes on 1.\n",
        "- basically yes/no type of function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELtPhp_81lxw"
      },
      "source": [
        "__Sigmoid function__\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.00.37%20PM.png?raw=1\">\n",
        "\n",
        "- a smooth function\n",
        "- very useful in the output layer when we try to predict probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7poUR1-i1lxx"
      },
      "source": [
        "__Rectifier__\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.06.12%20PM.png?raw=1\">\n",
        "\n",
        "- mostly used, very popular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bq2IZzG1lxz"
      },
      "source": [
        "__Hyperbolic Tangent (tanh)__\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.07.22%20PM.png?raw=1\">\n",
        "\n",
        "- similar to sigmoid function, but goes below 0 to -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNTOlsAa1lx0"
      },
      "source": [
        "Example: Assuming the dependent variable is binary (y=0 or 1), which activation function can we use? We have 2 options:\n",
        "    - Threshold function: It fits perfect when we need 0 or 1\n",
        "    - Sigmoid function: between 0 and 1, gives us the probability of 0 and 1.\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.12.41%20PM.png?raw=1\">\n",
        "\n",
        "- We have a very common combination where:\n",
        "    - Hidden layer: use rectifier function\n",
        "    - Output layer: use sigmoid function to give us the probabilities\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-11%20at%2010.59.53%20PM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJEhJid41lx2"
      },
      "source": [
        "## How do NNs work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kR2xi2br1lx3"
      },
      "source": [
        "Basic form of a neural network\n",
        "- Only contains an input layer and an output layer.\n",
        "- All of the input variables will be weighted up by the synapse and the price will be calculated by the weighted sum of the all inputs. You can use any activation functions to get a certain output.\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-15%20at%203.29.00%20PM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfH2CyGO1lx5"
      },
      "source": [
        "However, NN has an extra advantage that increases the accuracy which is hidden layers.\n",
        "\n",
        "For each neuron in the hidden layers:\n",
        "    - The weights of each input variables are not equal. Some weights may have non-zero values, some weights may have zero values. Because not all inputs are important for that neuron. For example: the first neuron only care about 2 inputs: area and distance from city, we can explain that the further from city, the area of the property is larger. That is why we don't need to draw the line of the synapses which are not important.\n",
        "    \n",
        "- Each one of the neuron cannot predict the price, but together they can do a proper job.\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-15%20at%203.48.32%20PM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnL26y91lx6"
      },
      "source": [
        "## How do Neural Networks learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RUsytYZ1lx8"
      },
      "source": [
        "- Perceptron: a single layer feed forward neural network\n",
        "- Output value ($\\hat y$): predicted value by the neural network\n",
        "- Actual value: y\n",
        "- We will calculate the cost function, which is the difference (error) between predicted value and actual value:\n",
        "    - Cost function: $\\frac{1}{2}(\\hat y - y)^2$\n",
        "    - Our goal is to minimize the cost fuction\n",
        "- After having the cost function, we will feed the information back to the neural network, then the weights get updated to minimize the cost function.\n",
        "- 1 epoch: is when we go through the whole dataset.\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-17%20at%209.24.54%20AM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "xzdeByHD1lx9"
      },
      "source": [
        "There are many cost functions. Here is [A list of cost functions used in neural networks, alongside applications](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZc66Df1lx-"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "BYJKcbYB1lx_"
      },
      "source": [
        "How can we minimize the cost function?\n",
        "- 1 approach is the brute force approach, when we try out lots of weights, then we have this graph:\n",
        "    - y-axis: cost function\n",
        "    - x-axis: $\\hat y$\n",
        "    \n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-17%20at%2010.59.44%20AM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8XfnJ7c1lyA"
      },
      "source": [
        "Why we should not use this brute-force approach by trying out lots of parameters and inputs for weights?\n",
        "- Because as you increase number of weights or synapses, you have to face the __curse of dimensionality__.\n",
        "- Example to understand the __curse of dimensionality__:\n",
        "    - We have a 1 layer neural network of 25 weights. If we need to try out 1000 combination of weights. We need: $1000^{25}= 10^{75}$ combinations.\n",
        "    - Sunway Taihulight: World's fastest super computer can do 93 PFLOPS (93 x $10^{15}$ floating operation/ second). It will take $10^{75} / 93 x 10^{15} = 1.08 * 10^{58} \\text{seconds} = 3.42 * 10^{50} \\text{years}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-G8yptV1lyC"
      },
      "source": [
        "We need a different approach: __Gradient Descent__\n",
        "\n",
        "__Intuition__:\n",
        "- Let's say we have a starting cost function. By looking at the angle of the cost function, we just need to differentiate, find out the slope is positive or negative, then we can decide to go downhill or uphill until we reach the minimum.\n",
        "- It is called gradient descent, because you are descending into the minimum of the cost function.\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-17%20at%2011.22.36%20AM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhfEsdTb1lyD"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB3kxlAt1lyF"
      },
      "source": [
        "__Gradient Descent__ requires the cost function to be a convex function. If our cost function is not convex, then our gradient descent will lead us to the local minimum instead of the global minimum.\n",
        "\n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-17%20at%2011.39.26%20AM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZxysRyR1lyG"
      },
      "source": [
        "However, __Stochastic Gradient Descent__ does not require our cost function to be convex.\n",
        "\n",
        "Differences between Gradient Descent (also called Batch Gradient Descent) and Stochastic Gradient Descent\n",
        "\n",
        "| __Gradient Descent__  | __Stochastic Gradient Descent__  |\n",
        "|:-:|:-:|\n",
        "| Calculate cost functions and adjust weights by taking all input values | Calculate the cost function and adjust the weight by looking at 1 row at a time |\n",
        "| runs lower  | runs faster  |\n",
        "| deterministic algorithm  | stochatic algorithm (random) |\n",
        "\n",
        "The reason why Stochastic Gradient Descent helps avoid the problem of stucking in local minimum is because Stochastic Gradient Descent has a much higher flunctuation. It is much more higher to find global minimum.\n",
        "\n",
        "__Additional Reading__:\n",
        "- [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](https://iamtrask.github.io/2015/07/27/python-network-part2/)\n",
        "- [Chapter 2 - How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln_Sb_qC1lyH"
      },
      "source": [
        "## Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZGtZQwF1lyI"
      },
      "source": [
        "__Forward Propagation__: Information is entered into the input layer and then it is propagated forward to get the output value $\\hat y$. The output values are then compared to actual values to compute errors. The errors are then back-propagated through the network in the opposite direction to train the network by adjusting the weights.\n",
        "\n",
        "Backpropagation allows us to adjust all the weights at the same time.\n",
        "\n",
        "__Additional Reading__: [Chapter 2 - How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0CYihK1lyJ"
      },
      "source": [
        "__Steps-by-steps walkthough in the training of ANN__\n",
        "    \n",
        "<img src=\"https://github.com/tuanavu/deep-learning-a-z/blob/master/DeepLearningA-Z/02-supervised-deep-learning/01-Artificial-Neural-Networks-ANN/images/Screen%20Shot%202017-06-17%20at%2012.04.33%20PM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN Example"
      ],
      "metadata": {
        "id": "NDYR1r2U2lfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=64, activation='relu', input_dim=8))  # Input layer with 8 input features\n",
        "model.add(Dense(units=32, activation='relu'))              # Hidden layer with 32 units\n",
        "model.add(Dense(units=1, activation='sigmoid'))            # Output layer with 1 unit (binary classification)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate some example data (replace this with your data)\n",
        "import numpy as np\n",
        "X_train = np.random.rand(100, 8)\n",
        "y_train = np.random.randint(2, size=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7xasNBd2yd8",
        "outputId": "9567a995-3dd3-4be9-da05-0d33f50d4261"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.5400\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.6200\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6759 - accuracy: 0.6000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.6100\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6653 - accuracy: 0.6200\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.6200\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6200\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6531 - accuracy: 0.6600\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6506 - accuracy: 0.6500\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.6400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae5e297dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial Neural Network (ANN) to predict the quality of wine using the popular Wine Quality dataset."
      ],
      "metadata": {
        "id": "22-0hmtr3Hnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "data = pd.read_csv(url, sep=';')\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data.drop('quality', axis=1)\n",
        "y = data['quality']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1))  # No activation function for regression\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Mean Squared Error on Test Set: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVqOjurm3Lqt",
        "outputId": "53345d1a-cf2f-43b7-a0e8-b6025c84537c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 1s 4ms/step - loss: 13.6189\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 2.5608\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 1.7633\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 1.4642\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 1.2340\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 1.0638\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.9125\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.8118\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7121\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.6376\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5743\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5309\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4865\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4536\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4337\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4147\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4059\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.3863\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.3830\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.3719\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3661\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3629\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3543\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3537\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3452\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3410\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3333\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3355\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3310\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3413\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3293\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3203\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3138\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3162\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3127\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3126\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3034\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3011\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3017\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2984\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2978\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2951\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2983\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2936\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2783\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2854\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2729\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2784\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2849\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2697\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3816\n",
            "Mean Squared Error on Test Set: 0.3816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN Chrun Modelling"
      ],
      "metadata": {
        "id": "yKain1tg4JOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "url ='https://raw.githubusercontent.com/ayushic2899/Deep-learning-Projects/main/ANN/Churn_Modelling.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "data = data.drop(columns=['RowNumber', 'CustomerId', 'Surname'])\n",
        "\n",
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data.drop('Exited', axis=1)\n",
        "y = data['Exited']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFouwJA94OBt",
        "outputId": "19cd53f3-d5b4-454c-bdd0-022375e70d07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "500/500 [==============================] - 2s 2ms/step - loss: 0.4431 - accuracy: 0.8166\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 1s 3ms/step - loss: 0.3679 - accuracy: 0.8524\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3478 - accuracy: 0.8584\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3420 - accuracy: 0.8579\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3388 - accuracy: 0.8587\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3357 - accuracy: 0.8609\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3332 - accuracy: 0.8611\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.8631\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8629\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8644\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3232 - accuracy: 0.8662\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3223 - accuracy: 0.8650\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3190 - accuracy: 0.8659\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3171 - accuracy: 0.8675\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3159 - accuracy: 0.8680\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 1s 3ms/step - loss: 0.3142 - accuracy: 0.8698\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3117 - accuracy: 0.8704\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3106 - accuracy: 0.8696\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3108 - accuracy: 0.8708\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.8726\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3056 - accuracy: 0.8724\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3035 - accuracy: 0.8725\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3018 - accuracy: 0.8739\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.3016 - accuracy: 0.8735\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2995 - accuracy: 0.8764\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2991 - accuracy: 0.8767\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2980 - accuracy: 0.8759\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2953 - accuracy: 0.8785\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 1s 3ms/step - loss: 0.2938 - accuracy: 0.8755\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2922 - accuracy: 0.8783\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2910 - accuracy: 0.8771\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2903 - accuracy: 0.8790\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2870 - accuracy: 0.8795\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2858 - accuracy: 0.8788\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2858 - accuracy: 0.8826\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2852 - accuracy: 0.8815\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2839 - accuracy: 0.8798\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2828 - accuracy: 0.8830\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2807 - accuracy: 0.8800\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.8824\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2777 - accuracy: 0.8831\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2768 - accuracy: 0.8826\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 1s 3ms/step - loss: 0.2741 - accuracy: 0.8809\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2742 - accuracy: 0.8854\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2723 - accuracy: 0.8860\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2715 - accuracy: 0.8840\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2693 - accuracy: 0.8860\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2696 - accuracy: 0.8879\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2686 - accuracy: 0.8882\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.2674 - accuracy: 0.8863\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8525\n",
            "Loss: 0.3689, Accuracy: 0.8525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "inferencing"
      ],
      "metadata": {
        "id": "S5esHpwO4_KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume the model is already trained and loaded\n",
        "\n",
        "# Load new data for inference (you can replace this with your own data)\n",
        "new_data = pd.DataFrame({\n",
        "    'CreditScore': [600],\n",
        "    'Age': [40],\n",
        "    'Tenure': [3],\n",
        "    'Balance': [60000],\n",
        "    'NumOfProducts': [2],\n",
        "    'HasCrCard': [1],\n",
        "    'IsActiveMember': [1],\n",
        "    'EstimatedSalary': [50000],\n",
        "    'Geography_Germany': [0],\n",
        "    'Geography_Spain': [1],\n",
        "    'Gender_Male': [0]\n",
        "})\n",
        "\n",
        "# Standardize the new data using the same scaler\n",
        "new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "# Perform inference\n",
        "predicted_prob = model.predict(new_data_scaled)\n",
        "predicted_class = (predicted_prob > 0.5).astype(int)  # Assuming threshold is 0.5\n",
        "\n",
        "if predicted_class[0][0] == 1:\n",
        "    print(\"The customer is predicted to churn.\")\n",
        "else:\n",
        "    print(\"The customer is predicted to stay.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTLdCcG05A4y",
        "outputId": "d4163a19-7926-4215-8a0b-c02d904c9284"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 90ms/step\n",
            "The customer is predicted to stay.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial Neural Network (ANN) to classify images from the Fashion MNIST dataset.\n",
        "\n",
        "Fashion MNIST dataset, which consists of grayscale images of 10 different fashion items. We preprocess the data by normalizing pixel values to the range [0, 1] and one-hot encoding the labels. Then, we create an ANN with a flattened input layer and two hidden layers. The output layer has 10 units (one for each class) with a softmax activation function. The model is compiled with categorical cross-entropy loss and accuracy as the metric. After training, we evaluate the model on the test set.\n"
      ],
      "metadata": {
        "id": "m8yIB6ic5fIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input data\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))  # Output layer with 10 units for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1NvX1FN5hvJ",
        "outputId": "c2f89a76-1ad6-4184-968d-6f66742fcedb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 8s 4ms/step - loss: 0.5163 - accuracy: 0.8186 - val_loss: 0.4250 - val_accuracy: 0.8442\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3807 - accuracy: 0.8632 - val_loss: 0.3585 - val_accuracy: 0.8690\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3428 - accuracy: 0.8741 - val_loss: 0.3498 - val_accuracy: 0.8696\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3155 - accuracy: 0.8832 - val_loss: 0.3449 - val_accuracy: 0.8758\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3009 - accuracy: 0.8877 - val_loss: 0.3365 - val_accuracy: 0.8769\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2861 - accuracy: 0.8927 - val_loss: 0.3272 - val_accuracy: 0.8820\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2725 - accuracy: 0.8966 - val_loss: 0.3126 - val_accuracy: 0.8902\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2610 - accuracy: 0.9024 - val_loss: 0.3260 - val_accuracy: 0.8869\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2503 - accuracy: 0.9064 - val_loss: 0.3291 - val_accuracy: 0.8853\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2433 - accuracy: 0.9083 - val_loss: 0.3155 - val_accuracy: 0.8883\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3468 - accuracy: 0.8795\n",
            "Loss: 0.3468, Accuracy: 0.8795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"fashion_mnist_model.h5\")\n",
        "\n",
        "# Load the saved model\n",
        "saved_model = load_model(\"fashion_mnist_model.h5\")\n",
        "\n",
        "# Load new data for inference (you can replace this with your own data)\n",
        "new_data = X_test[:5]  # Use the first 5 test images\n",
        "new_data = np.expand_dims(new_data, axis=-1)  # Add a channel dimension\n",
        "\n",
        "# Perform inference using the saved model\n",
        "predictions = saved_model.predict(new_data)\n",
        "\n",
        "# Get the class with the highest probability for each prediction\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Map class indices to actual class labels\n",
        "class_labels = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n",
        "\n",
        "# Print the predicted class labels\n",
        "for i, predicted_class in enumerate(predicted_classes):\n",
        "    print(f\"Image {i+1} - Predicted: {class_labels[predicted_class]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0y4XMIV6QTe",
        "outputId": "628f9353-9d42-4da2-c680-8b8d4d66360c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 181ms/step\n",
            "Image 1 - Predicted: Ankle boot\n",
            "Image 2 - Predicted: Pullover\n",
            "Image 3 - Predicted: Trouser\n",
            "Image 4 - Predicted: Trouser\n",
            "Image 5 - Predicted: Shirt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial Neural Network (ANN) to classify images of handwritten digits from the MNIST dataset using Python and TensorFlow"
      ],
      "metadata": {
        "id": "mUSOjbf_7FZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input data\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))  # Output layer with 10 units for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8Af4SLm7I-f",
        "outputId": "50d3248b-dd69-4ee8-89b4-5c38abdeb415"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2627 - accuracy: 0.9242 - val_loss: 0.1250 - val_accuracy: 0.9634\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1079 - accuracy: 0.9673 - val_loss: 0.1019 - val_accuracy: 0.9703\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0751 - accuracy: 0.9772 - val_loss: 0.1019 - val_accuracy: 0.9701\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0836 - val_accuracy: 0.9751\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0446 - accuracy: 0.9852 - val_loss: 0.0916 - val_accuracy: 0.9747\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0351 - accuracy: 0.9886 - val_loss: 0.0875 - val_accuracy: 0.9759\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0302 - accuracy: 0.9900 - val_loss: 0.0933 - val_accuracy: 0.9749\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.1100 - val_accuracy: 0.9732\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.1172 - val_accuracy: 0.9737\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.1172 - val_accuracy: 0.9730\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1029 - accuracy: 0.9759\n",
            "Loss: 0.1029, Accuracy: 0.9759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save previous model and do Inferencing"
      ],
      "metadata": {
        "id": "B1htF9w87Ww-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save(\"mnist_digits_model.h5\")\n",
        "\n",
        "# Load the saved model\n",
        "saved_model = load_model(\"mnist_digits_model.h5\")\n",
        "\n",
        "# Load new data for inference (you can replace this with your own data)\n",
        "new_data = X_test[:5]  # Use the first 5 test images\n",
        "\n",
        "# Perform inference using the saved model\n",
        "predictions = saved_model.predict(new_data)\n",
        "\n",
        "# Get the class with the highest probability for each prediction\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the predicted class labels\n",
        "for i, predicted_class in enumerate(predicted_classes):\n",
        "    print(f\"Image {i+1} - Predicted digit: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LspWqmm47ZW6",
        "outputId": "10745b2e-e39a-40ef-8382-8b09594a9cc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n",
            "Image 1 - Predicted digit: 7\n",
            "Image 2 - Predicted digit: 2\n",
            "Image 3 - Predicted digit: 1\n",
            "Image 4 - Predicted digit: 0\n",
            "Image 5 - Predicted digit: 4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "103px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": true,
      "toc_number_sections": false,
      "toc_section_display": "block",
      "toc_threshold": "8",
      "toc_window_display": true,
      "widenNotebook": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}